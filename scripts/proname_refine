#!/bin/bash

echo -ne "\n**************************************************************************************************************\n"
echo -ne "* You are executing the \033[1mproname_refine\033[0m script, which is the third part of the \033[1mPRONAME\033[0m pipeline.              *\n"
echo -ne "*                                                                                                            *\n"
echo -ne "* Please \033[31m\033[1mdo not\033[0m move, rename or remove files and folders generated by PRONAME until the end of the pipeline. *\n"
echo -ne "**************************************************************************************************************\n\n"

# Help menu

Help()
{

## Display Help
   echo -ne "\nThe proname_refine script is the third part of the PRONAME pipeline. This script carries out the clustering of high-quality sequence data coming from proname_filter, \nand the subsequent polishing with a tool dedicated to Nanopore data (i.e. medaka). Optionally, the script can import the generated files into QIIME2 for further processing."
   echo
   echo -ne "\nSyntax: proname_refine [--clusteringmethod|--clusterid|--clusterthreads|--inputpath|--subsampledreads|--medakabatchsize|--medakathreads|--medakamodel|--chimeramethod|--chimeradb|--qiime2import|--version|--verbose|--help]"
   echo
   echo "Options:"
   echo -ne "\t--clusteringmethod\tThe tool used to cluster sequences. It can be either \"vsearch\" or \"mmseqs2\". VSEARCH offers \n"
   echo -ne "\t\t\t\thigh accuracy and is well-suited for small to medium datasets, but can be slower on large datasets. MMseqs2 is \n"
   echo -ne "\t\t\t\tslightly less accurate but dramatically faster, especially on large datasets. [Options: \"vsearch\" or \n"
   echo -ne "\t\t\t\t\"mmseqs2\", Default: \"vsearch\"] \n"
   echo
   echo -ne "\t--clusterid\t\tThe percentage of identity at which clustering should be performed. [Options: decimal between 0 and 1]\n"
   echo
   echo -ne "\t--clusterthreads\tNumber of threads to use for the clustering step. You can know the number of available threads on your computer \n"
   echo -ne "\t\t\t\tby running the command 'nproc --all' [Default: 2]\n"
   echo
   echo -ne "\t--inputpath\t\tPath to the folder containing raw fastq files. This must be the same path than the one provided while \n"
   echo -ne "\t\t\t\trunning proname_import and proname_filter.\n"
   echo
   echo -ne "\t--subsampledreads\tNumber of subsampled reads that will be aligned against the centroid sequence during polishing. [Default: 300]\n"
   echo
   echo -ne "\t--medakabatchsize\tControls memory use. The default value has been set to 100. If Medaka shows out of memory errors, \n"
   echo -ne "\t\t\t\tthe batch size should be reduced. [Default: 100]\n"
   echo
   echo -ne "\t--medakathreads\t\tNumber of threads to use for the polishing step. If running Medaka on a CPU, it is recommended to \n"
   echo -ne "\t\t\t\tset this value to the maximum number of available threads. PRONAME will automatically split the polishing process \n"
   echo -ne "\t\t\t\tinto multiple parallel jobs to significantly increase the analysis speed. Each job uses 2 threads for Medaka, with \n"
   echo -ne "\t\t\t\t2 additional threads allocated to each job for parallel overhead. \n"
   echo -ne "\t\t\t\tIf running on a GPU, this setting has little impact, as the main computation is handled by PyTorch and CUDA. [Default: 4]\n"
   echo
   echo -ne "\t--medakamodel\t\tBasecalling model used to generate raw fastq files. This model will be used by medaka to polish data.\n"
   echo -ne "\t\t\t\tThe list of available models can be found by running 'medaka tools list\_models'\n"
   echo
   echo -ne "\t--chimeramethod\t\tSpecify the chimera detection method: \"ref\" (reference-based) or \"denovo\" (de novo detection). [Default: \"ref\"]\n"
   echo
   echo -ne "\t--chimeradb\t\tPath to the reference database to use for the chimera detection. This argument is required only \n"
   echo -ne "\t\t\t\tif --chimeramethod = \"ref\" (reference-based chimera detection).\n"
   echo
   echo -ne "\t--qiime2import\t\tIndicate whether the generated representative sequences and table must be imported into QIIME2. [Options: \"yes\" or \"no\"]\n"
   echo
   echo -ne "\t--deletefiles\t\tDelete all non-essential files, i.e. files generated with proname_filter that are no more needed for the rest of \n"
   echo -ne "\t\t\t\tthe analysis through PRONAME. [Options: \"yes\" or \"no\", Default: no] \n"
   echo
   echo -ne "\t--version\t\tPrint the version of the pipeline.\n"
   echo
   echo -ne "\t--verbose\tActivate verbose/debug mode (no redirections).\n"
   echo
   echo -ne "\t--help\t\t\tPrint this help."
   echo


## Usage example
   echo -ne "\nUsage example:"
   echo -ne "\n-------------"
   echo
   echo "proname_refine \\"
   echo "  --clusterid 0.90 \\"
   echo "  --inputpath RawData \\"
   echo "  --medakamodel r1041_e82_400bps_sup_v5.2.0 \\"
   echo "  --chimeradb /opt/db/rEGEN-B/rEGEN-B_sequences.fasta \\"
   echo "  --qiime2import yes"
   echo
}

Version()
{
    echo -ne "proname_refine from the PRONAME pipeline, version 2.1.3\n\n"
}


# Variable default values
clustering_threads="2"
n_subsampled_reads="300"
medaka_batchsize="100"
medaka_threads="4"
delete_non_essential="no"
clustering_method="vsearch"
chimera_method="ref"
verbose=false


# Transform long options to short ones

for arg in "$@"; do
  shift
  case "$arg" in
    '--help')               set -- "$@" '-h'   ;;
    '--version')            set -- "$@" '-v'   ;;
    '--clusterid')          set -- "$@" '-a'   ;;
    '--clusterthreads')     set -- "$@" '-b'   ;;
    '--inputpath')          set -- "$@" '-c'   ;;
    '--subsampledreads')    set -- "$@" '-d'   ;;
    '--medakabatchsize')    set -- "$@" '-e'   ;;
    '--medakathreads')      set -- "$@" '-f'   ;;
    '--medakamodel')        set -- "$@" '-g'   ;;
    '--chimeradb')          set -- "$@" '-i'   ;;
    '--qiime2import')       set -- "$@" '-j'   ;;
    '--clusteringmethod')   set -- "$@" '-k'   ;;
    '--chimeramethod')      set -- "$@" '-l'   ;;
    '--deletefiles')        set -- "$@" '-r'   ;;
    '--verbose')            verbose=true ;;
    *)                      set -- "$@" "$arg" ;;
  esac
done


# Parse short options

while getopts :hva:b:c:d:e:f:g:i:j:k:l:r: flag
do
    case "${flag}" in
        h) Help
        exit;;
        v) Version
        exit;;
        a) clustering_id=${OPTARG};;
        b) clustering_threads=${OPTARG};;
        c) fastq_folder=${OPTARG};;
        d) n_subsampled_reads=${OPTARG};;
        e) medaka_batchsize=${OPTARG};;
        f) medaka_threads=${OPTARG};;
        g) medaka_model=${OPTARG};;
        i) chimera_db=${OPTARG};;
        j) qiime2_import=${OPTARG};;
        k) clustering_method=${OPTARG};;
        l) chimera_method=${OPTARG};;
        r) delete_non_essential=${OPTARG};;
        \?) # Invalid option
                echo -ne "Error: Invalid option\n"
                echo -ne "Please consult the help menu with 'proname_refine --help'\n"
                exit;;
    esac
done


# Mandatory arguments

if [ ! "$clustering_id" ] || [ ! "$fastq_folder" ] || [ ! "$medaka_model" ] || [ ! "$qiime2_import" ]
then
   echo -ne "\n\033[31m\033[1mError: arguments --clusterid, --inputpath, --medakamodel, --chimeradb and --qiime2import must be provided\033[0m\n\n"
   echo -ne "Please consult the help menu with 'proname_refine --help'\n"
   exit 1
fi

if { [ "$chimera_method" = "ref" ] && [ ! "$chimera_db" ]; }
then
   echo -ne "\n\033[31m\033[1mError: The argument --chimeradb must be provided if --chimeramethod = \"ref\" (reference-based chimera detection).\033[0m\n\n"
   echo -ne "Please consult the help menu with 'proname_refine --help'\n"
   exit 1
fi

if [[ "$chimera_method" != "ref" && "$chimera_method" != "denovo" ]]
then
   echo -ne "\n\033[31m\033[1mError: the value of the --chimeramethod argument must be 'ref' or 'denovo'.\033[0m\n\n"
   echo -ne "Please consult the help menu with 'proname_refine --help'\n"
   exit 1
fi

if [[ "$clustering_method" != "vsearch" && "$clustering_method" != "mmseqs2" ]]
then
   echo -ne "\n\033[31m\033[1mError: the value of the --clusteringmethod argument must be 'vsearch' or 'mmseqs2'.\033[0m\n\n"
   echo -ne "Please consult the help menu with 'proname_refine --help'\n"
   exit 1
fi


# Define exec_cmd function for conditional verbosity
exec_cmd() {
  if $verbose; then
    eval "$@"
  else
    eval "$@" >/dev/null 2>&1
  fi
}

######################################################

# Listing the sequence identifiers present in each fastq file

if [ ! -d "Rawseqids" ]
then
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mRecording read distribution in each sample...\033[0m\n"

   mkdir Rawseqids

   for file in ${fastq_folder}/*.fastq
   do
      sample=$(basename "$file" .fastq)
      awk 'NR == 1 || (NR-1) % 4 == 0 {sub(/^@/,""); print $0}' "$file" > Rawseqids/rawseqids_${sample}
   done

   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mRead distribution recorded! \033[0m\n"
fi


# Storing in variables the path of files from proname_filter

## File from proname_filter with duplex data
if [ -e "HQ/HQ_duplex_seqs.fastq" ] && [ ! -e "HQ/HQ_simplex_duplex_seqs.fastq" ]
then
  HQ_fastq="HQ/HQ_duplex_seqs.fastq"
fi

## File from proname_filter with simplex data
if [ -e "HQ/HQ_simplex_seqs.fastq" ] && [ ! -e "HQ/HQ_simplex_duplex_seqs.fastq" ]
then
  HQ_fastq="HQ/HQ_simplex_seqs.fastq"
fi

## File from proname_filter with simplex and duplex data
if [ -f "HQ/HQ_simplex_duplex_seqs.fastq" ]
then
  HQ_fastq="HQ/HQ_simplex_duplex_seqs.fastq"
fi

# Removing items from a potential previous failed run

rm -rf vsearch_clusters
rm -rf mmseqs2_clusters
rm -f cluster*_seq_count

# Clustering reads into OTUs

echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mClustering reads...\033[0m\n"

if [ ${clustering_method} = "vsearch" ]
then
   mkdir vsearch_clusters

   vsearch \
     --cluster_fast ${HQ_fastq} \
     --id ${clustering_id} \
     --strand both \
     --threads ${clustering_threads} \
     --clusters vsearch_clusters/cluster

   exec_cmd "remove_singletons.py vsearch_clusters"


   # Recording clustered reads according to their sample provenance

   cluster_reads_count.py "${clustering_method}_clusters" "${fastq_folder}" "Rawseqids"

   if find vsearch_clusters -name 'cluster*' | grep -q . 1> /dev/null 2>&1
   then
      echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mRead clustering completed! \033[0m\n"
   else
      echo -ne "\n\033[31m\033[1mError: read clustering failed. \033[0m\n\n"
      echo -ne "Please consult the help menu.\n"
      exit;
   fi

   # Splitting clusters into 2 files

   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mSplitting clusters into two files...\033[0m\n"

   for i in vsearch_clusters/*;do seqkit split -p 2 $i;done

   if find vsearch_clusters -name '*split*' | grep -q . 1> /dev/null 2>&1
   then
      echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mSplitting completed! \033[0m\n"
   else
      echo -ne "\n\033[31m\033[1mError: cluster splitting failed. \033[0m\n\n"
      echo -ne "Please consult the help menu.\n"
      exit;
   fi

   # Extracting centroid sequences

   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mExtracting centroid sequences...\033[0m\n"

   for i in $(ls vsearch_clusters | grep "split" | cut -d "." -f 1)
   do
      seqkit range -r -1:-1 vsearch_clusters/${i}.split/${i}.part_001 -w 0 -o vsearch_clusters/centroid_${i}.fasta
   done

   if find vsearch_clusters -name 'centroid*' | grep -q . 1> /dev/null 2>&1
   then
      echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mExtraction completed! \033[0m\n"
   else
      echo -ne "\n\033[31m\033[1mError: centroid extraction failed. \033[0m\n\n"
      echo -ne "Please consult the help menu.\n"
      exit;
   fi
elif [ ${clustering_method} = "mmseqs2" ]
then
   mkdir mmseqs2_clusters

   seqkit fq2fa "$HQ_fastq" -o "${HQ_fastq%.fastq}.fasta"

   export HQ_fastq="${HQ_fastq%.fastq}.fasta"

   mmseqs easy-cluster "${HQ_fastq}" output tmp --min-seq-id ${clustering_id} --threads ${clustering_threads} 2> /dev/null

   # Extracting clusters and centroid sequences

   extract_mmseqs2_clusters.py

   # Removing singletons

   exec_cmd "remove_singletons.py mmseqs2_clusters"

   # Recording clustered reads according to their sample provenance

   cluster_reads_count.py "${clustering_method}_clusters" "${fastq_folder}" "Rawseqids"

   if find mmseqs2_clusters -name 'cluster*' | grep -q . 1> /dev/null 2>&1
   then
      echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mRead clustering completed! \033[0m\n"
   else
      echo -ne "\n\033[31m\033[1mError: read clustering failed. \033[0m\n\n"
      echo -ne "Please consult the help menu.\n"
      exit;
   fi
   rm -r tmp
   rm output_*
fi

# Randomly subsampling N reads

echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mSubsampling reads in each cluster...\033[0m\n"

for i in $(ls ${clustering_method}_clusters | grep "centroid" | cut -d '_' -f 2 | cut -d '.' -f 1)
do
   seqkit shuffle -s 23 ${clustering_method}_clusters/$i 2>/dev/null | seqkit head -n ${n_subsampled_reads} -w 0 -o ${clustering_method}_clusters/subreads_${i}.fasta
done

# Adding medaka model in sequence headers
for fasta in ${clustering_method}_clusters/{subreads_cluster*.fasta,centroid_*.fasta}
do
   sed -i "s/^>\(.*\)/>\1\tmv:${medaka_model}/" "$fasta"
done

if find ${clustering_method}_clusters -name 'subreads*' | grep -q . 1> /dev/null 2>&1
then
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mSubsampling completed! \033[0m\n"
else
   echo -ne "\n\033[31m\033[1mError: read subsampling failed. \033[0m\n\n"
   echo -ne "Please consult the help menu.\n"
   exit;
fi


# Defining the global function for parallel polishing
medaka_polishing() {
   local medaka_threads="${medaka_threads}"
   local medaka_model="${medaka_model}"
   local medaka_batchsize="${medaka_batchsize}"
   local clustering_method="${clustering_method}"

   # Generating consensus sequences

   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mCarrying out medaka polishing to generate consensus sequences...\033[0m\n"

   # Computing the number of parallel jobs
   local PARALLEL_JOBS=$(( medaka_threads / 4 ))

   if [ "${PARALLEL_JOBS}" -lt 1 ]
   then
      PARALLEL_JOBS=1
   fi
   echo "Launching polishing in parallel using ${PARALLEL_JOBS} jobs (4 threads per job)."

   # Generating cluster IDs
   find ${clustering_method}_clusters -type f -name "subreads_cluster*.fasta" | \
      xargs -n1 basename | cut -d '_' -f 2 | cut -d '.' -f 1 > cluster_ids.txt

   if [ ! -s cluster_ids.txt ]
   then
      echo "Error: No clusters found!"
      exit 1
   fi

   # Exporting variables for parallel
   export medaka_model
   export medaka_batchsize
   export clustering_method

   # Polishing function
   polish_cluster() {
      local cluster_id="$1"

      medaka_consensus \
         -d ${clustering_method}_clusters/centroid_${cluster_id}.fasta \
         -i ${clustering_method}_clusters/subreads_${cluster_id}.fasta \
         -b "${medaka_batchsize}" \
         -t 2 \
         -m "${medaka_model}" \
         -o ${clustering_method}_clusters/medaka_${cluster_id}

   }
   export -f polish_cluster

   parallel -j "${PARALLEL_JOBS}" polish_cluster :::: cluster_ids.txt

   rm cluster_ids.txt
}

# Calling polishing function
medaka_polishing

#############################

if find ${clustering_method}_clusters -name 'medaka*' | grep -q . 1> /dev/null 2>&1
then
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mPolishing completed! \033[0m\n"
else
   echo -ne "\n\033[31m\033[1mError: polishing failed. \033[0m\n\n"
   echo -ne "Please consult the help menu.\n"
   exit;
fi

# Creating a linking table between clusters and their corresponding representative seqid

export clustering_method
export medaka_threads

find "${clustering_method}_clusters" -type f -name "consensus.fasta" -path "*medaka_*" > consensus_paths.txt
parallel --jobs "$medaka_threads" --no-run-if-empty '
    i=$(basename $(dirname {}) | cut -d "_" -f 2)
    rep_seq=$(grep ">" {} | cut -d ">" -f 2)
    printf "%s\t%s\n" "$i" "$rep_seq"
' :::: consensus_paths.txt > clusters_seqids_linking_table
rm consensus_paths.txt


# Creating the feature table
## Adding a header line in each seq_count file

echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mCreating a feature table...\033[0m\n"

add_header() {
    sample_id=$(basename "$1" _seq_count)
    header=$(awk -v id="$sample_id" '$1==id {print $2}' clusters_seqids_linking_table)
    { echo -e "Feature_ID\t$header"; cat "$1"; } > "${1}_tmp" && mv "${1}_tmp" "$1"
}
export -f add_header

find . -maxdepth 1 -type f -name "*_seq_count" > seq_count_files.txt
parallel --jobs "$medaka_threads" --no-run-if-empty add_header :::: seq_count_files.txt
rm seq_count_files.txt


## Merging all seq_count files

find . -maxdepth 1 -type f -name "*_seq_count" -exec cut -f1 {} \; | LC_ALL=C sort -u > Global_table

for f in *_seq_count
do
   LC_ALL=C sort -t $'\t' -k1,1 "$f" > "${f}_sorted"
   mv "${f}_sorted" "$f"
done

merge_column() {
   LC_ALL=C join -t $'\t' -a1 -e 0 -o auto Global_table "$1" > Global_table.tmp && mv Global_table.tmp Global_table
}
export -f merge_column

for f in *_seq_count
do
   merge_column "$f"
done

sed -n '/^Feature_ID/p' Global_table > Global_table_new
sed '/^Feature_ID/d' Global_table >> Global_table_new
mv Global_table_new Global_table

## Inverting lines and columns

awk '
{
    for (i=1; i<=NF; i++) {
        a[i, NR] = $i
    }
}
END {
    for (i=1; i<=NF; i++) {
        for (j=1; j<=NR; j++) {
            printf "%s%s", a[i,j], (j<NR ? "\t" : "\n")
        }
    }
}
' Global_table > Global_table_tmp && mv Global_table_tmp Global_table

if [ -e "Global_table" ]
then
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mFeature table created! \033[0m\n"
else
   echo -ne "\n\033[31m\033[1mError: the creation of the feature table failed. \033[0m\n\n"
   echo -ne "Please consult the help menu.\n"
   exit;
fi

# Removing chimera sequences

echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mRemoving chimera sequences...\033[0m\n"

find "${clustering_method}_clusters/" -path "${clustering_method}_clusters/medaka*/consensus.fasta" -exec cat {} + > medaka_consensus_seqs.fasta

if [ "$chimera_method" = "ref" ]
then
   vsearch \
      --uchime_ref medaka_consensus_seqs.fasta \
      --db ${chimera_db} \
      --nonchimeras rep_seqs.fasta
elif [ "$chimera_method" = "denovo" ]
then
   vsearch \
      --uchime_denovo medaka_consensus_seqs.fasta \
      --nonchimeras rep_seqs.fasta
fi


if [ -e "rep_seqs.fasta" ]
then
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mChimera sequences removed! \033[0m\n"
else
   echo -ne "\n\033[31m\033[1mError: the creation of the feature table failed. \033[0m\n\n"
   echo -ne "Please consult the help menu.\n"
   exit;
fi

# Checking that every nucleotide is uppercase

awk '{if ($0 ~ /^>/) {print $0} else {print toupper($0)}}' rep_seqs.fasta > rep_seqs.fasta_tmp
mv rep_seqs.fasta_tmp rep_seqs.fasta

echo -ne "$(head -n 1 Global_table)\n$(grep -f <(grep ">" rep_seqs.fasta | cut -d ">" -f 2) Global_table)\n" > rep_table.tsv


# Clean-up

if [ ${delete_non_essential} = "yes" ]
then
   find . -maxdepth 1 -type f -name "*_seq_count" -exec rm {} +
else
   mkdir -p cluster_seq_count_folder
   find . -maxdepth 1 -type f -name "*_seq_count" -exec mv {} cluster_seq_count_folder \;
fi

rm clusters_seqids_linking_table
rm Global_table
rm medaka_consensus_seqs.fasta


# Data import into QIIME2

if [ ${qiime2_import} = "yes" ]
then
   ARCH=$(uname -m)

   if [[ "$ARCH" == "x86_64" ]]
   then
      # Data import into QIIME2
      echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;38;5;39mImporting representative sequences and table into QIIME2...\033[0m\n"

      biom convert -i rep_table.tsv -o rep_table.biom --table-type "OTU table" --to-hdf5

      exec_cmd "qiime tools import \
        --input-path rep_table.biom \
        --type 'FeatureTable[Frequency]' \
        --input-format BIOMV210Format \
        --output-path rep_table.qza"

      rm rep_table.biom

      exec_cmd "qiime tools import \
        --type FeatureData[Sequence] \
        --input-path rep_seqs.fasta \
        --output-path rep_seqs.qza"

      # Creation of the metadata table
      {
         echo -e "sample-id\tsample-name"
         echo -e "#q2:types\tcategorical"
         find "${fastq_folder}" -maxdepth 1 -type f -name "*.fastq" -exec basename {} .fastq \; \
            | sort \
            | while IFS= read -r s; do
                 printf "%s\t%s\n" "$s" "$s"
              done
      } > sample_metadata.tsv


      if [ -e "rep_table.qza" ] && [ -e "rep_seqs.qza" ]
      then
         echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : \033[1;32mImport completed! \033[0m\n"
      else
         echo -ne "\n\033[31m\033[1mError: data import into QIIME2 failed. \033[0m\n\n"
         echo -ne "Please consult the help menu.\n"
         exit;
      fi

      echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : The next and last script of the pipeline to use is \033[1mproname_taxonomy\033[0m.\n\n"
      echo -ne "\nUsage example:"
      echo -ne "\n-------------"
      echo
      echo "proname_taxonomy \\"
      echo "  --qseqs rep_seqs.qza \\"
      echo "  --qtable rep_table.qza \\"
      echo "  --db /opt/db/rEGEN-B/rEGEN-B_sequences.fasta \\"
      echo "  --reftax /opt/db/rEGEN-B/rEGEN-B_taxonomy.tsv \\"
      echo "  --metadata sample_metadata.tsv \\"
      echo "  --assay assay1 \\"
      echo "  --phyloseq yes"
      echo

   elif [[ "$ARCH" == "aarch64" ]]
   then
      biom convert -i rep_table.tsv -o rep_table.biom --table-type "OTU table" --to-hdf5
      
      # Creation of the metadata table
      {
         echo -e "sample-id\tsample-name"
         echo -e "#q2:types\tcategorical"
         find "${fastq_folder}" -maxdepth 1 -type f -name "*.fastq" -exec basename {} .fastq \; \
            | sort \
            | while IFS= read -r s; do
                 printf "%s\t%s\n" "$s" "$s"
              done
      } > sample_metadata.tsv

      echo -ne "\n\033[1;33m!!! NOTE FOR ARM64 USERS !!!\033[0m\n"
      echo -ne "The current Docker image runs on ARM64 architecture, and QIIME2 is not natively supported on ARM64.\n\n"
      echo -ne "\e[4mYou have two options\e[0m:\n\n"
      echo -ne "(i) If QIIME2 is installed on your host machine, run the following commands manually on it:\n\n"
      echo -ne "\033[1m# Importing feature table into QIIME2\033[0m\n"
      echo -ne "qiime tools import --input-path rep_table.biom --type 'FeatureTable[Frequency]' --input-format BIOMV210Format --output-path rep_table.qza\n\n"
      echo -ne "\033[1m# Importing representative sequences into QIIME2\033[0m\n"
      echo -ne "qiime tools import --type FeatureData[Sequence] --input-path rep_seqs.fasta --output-path rep_seqs.qza\n\n"
      echo
      echo -ne "(ii) If QIIME2 is not installed on your host machine, run the following commands manually on it:\n\n"
      echo -ne "\033[1m# Importing feature table into QIIME2\033[0m\n"
      echo -ne "docker run --rm -v /path/to/host/data:/data quay.io/qiime2/core:2024.5 qiime tools import \\ \n"
      echo -ne "  --input-path rep_table.biom \\ \n"
      echo -ne "  --type 'FeatureTable[Frequency]' \\ \n"
      echo -ne "  --input-format BIOMV210Format \\ \n"
      echo -ne "  --output-path rep_table.qza \n"
      echo
      echo -ne "\033[1m# Importing representative sequences into QIIME2\033[0m\n"
      echo -ne "docker run --rm -v /path/to/host/data:/data quay.io/qiime2/core:2024.5 qiime tools import \\ \n"
      echo -ne "  --type FeatureData[Sequence] \\ \n"
      echo -ne "  --input-path rep_seqs.fasta \\ \n"
      echo -ne "  --output-path rep_seqs.qza \n\n"

      echo -ne "\033[1;33mOnce these files are ready, you can proceed to run \033[1mproname_taxonomy\033[0m.\033[0m\n\n"
      echo -ne "\nUsage example:"
      echo -ne "\n-------------"
      echo
      echo "proname_taxonomy \\"
      echo "  --qseqs rep_seqs.qza \\"
      echo "  --qtable rep_table.qza \\"
      echo "  --db /opt/db/rEGEN-B/rEGEN-B_sequences.fasta \\"
      echo "  --reftax /opt/db/rEGEN-B/rEGEN-B_taxonomy.tsv \\"
      echo "  --metadata sample_metadata.tsv \\"
      echo "  --assay assay1 \\"
      echo "  --phyloseq yes"
      echo
   
   else
      echo -ne "\n\033[31m\033[1mError: Unknown system architecture '$ARCH'. Unable to decide QIIME2 strategy.\033[0m\n"
      exit 1
   fi

else
   echo -ne "[$(date +'%Y-%m-%d || %H:%M:%S')] : The next and last script of the pipeline to use is \033[1mproname_taxonomy\033[0m.\n\n"
   echo -ne "\nUsage example:"
   echo -ne "\n-------------"
   echo
   echo "proname_taxonomy \\"
   echo "  --qseqs rep_seqs.fasta \\"
   echo "  --qtable rep_table.tsv \\"
   echo "  --db /opt/db/rEGEN-B/rEGEN-B_sequences.fasta \\"
   echo "  --reftax /opt/db/rEGEN-B/rEGEN-B_taxonomy.tsv \\"
   echo "  --assay assay1 \\"
   echo "  --phyloseq yes"
   echo
fi

if [ ${delete_non_essential} = "yes" ]
then
    rm *read_distribution.tsv
    rm LengthvsQualityScatterPlot*
    rm -r HQ
    rm -r ${clustering_method}_clusters
fi


unset HQ_fastq
unset clustering_method
unset clustering_id
unset clustering_threads
unset fastq_folder
unset n_subsampled_reads
unset medaka_batchsize
unset medaka_threads
unset medaka_model
unset chimera_db
unset qiime2_import
unset delete_non_essential
unset chimera_method
unset ARCH
unset verbose
